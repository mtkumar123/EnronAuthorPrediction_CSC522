{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb0fdded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/manoj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from collections import Counter \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ddc0706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put in the file path to the dataset created from extractingauthors.ipynb\n",
    "df = pd.read_csv(\"./enron.csv\")\n",
    "df = df.drop([\"Email Folder\"], axis=1)\n",
    "#We need only the top 20 authors ordered by number of emails found in either the\n",
    "#sent folder or _sent_mail folder\n",
    "\n",
    "#Add top_authors = df.value_counts([\"Folder\"])[:X] for the number of authors required\n",
    "# Change X to 5,10,15 to test with 5, 10, 15 authors\n",
    "top_authors = df.value_counts([\"Folder\"])[:5]\n",
    "df = df.loc[df[\"Folder\"].isin(list(top_authors.index.get_level_values(0)))].drop([\"Unnamed: 0\"], axis=1).reset_index(drop=True)\n",
    "df = df[df[\"Text\"]!=\" \"]\n",
    "df = df[df[\"Text\"]!=\"\\n\"]\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5fadcc27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mann-k          8167\n",
       "kaminski-v      5926\n",
       "dasovich-j      4805\n",
       "germany-c       4571\n",
       "shackleton-s    4003\n",
       "Name: Folder, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Folder\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a581c1",
   "metadata": {},
   "source": [
    "Here we are using the enron.csv file created from the noteboook \"STEP2_ExtractingAuthorEmails.ipynb\". This csv contains all the sent emails from 20 authors, and the extracted body of text from each email. You can change the X value too 5,10,15 to test with the corresponding number of authors. Here we are testing with 5 authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbe3313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_distribution(samples_per_author, df):\n",
    "    df3 = pd.DataFrame(columns=[\"Author\", \"Folder\", \"File\", \"Text\", \"Raw Text\"]) \n",
    "    for folder in df[\"Folder\"].value_counts().index:\n",
    "        df3 = df3.append(df[df[\"Folder\"]==folder].sample(n=samples_per_author), ignore_index=True)\n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7d7f2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mann-k          4000\n",
      "kaminski-v      4000\n",
      "dasovich-j      4000\n",
      "germany-c       4000\n",
      "shackleton-s    4000\n",
      "Name: Folder, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Folder</th>\n",
       "      <th>File</th>\n",
       "      <th>Text</th>\n",
       "      <th>Raw Text</th>\n",
       "      <th>Message ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kay</td>\n",
       "      <td>mann-k</td>\n",
       "      <td>543.0</td>\n",
       "      <td>Here is the latest version.  Our meeting is sc...</td>\n",
       "      <td>Message-ID: &lt;22360372.1075845931625.JavaMail.e...</td>\n",
       "      <td>22360372.1075845931625.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kay</td>\n",
       "      <td>mann-k</td>\n",
       "      <td>67.0</td>\n",
       "      <td>Per your request.\\n</td>\n",
       "      <td>Message-ID: &lt;2759744.1075845920112.JavaMail.ev...</td>\n",
       "      <td>2759744.1075845920112.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kay</td>\n",
       "      <td>mann-k</td>\n",
       "      <td>2643.0</td>\n",
       "      <td>The word is that giving the notice could put t...</td>\n",
       "      <td>Message-ID: &lt;30826773.1075846093464.JavaMail.e...</td>\n",
       "      <td>30826773.1075846093464.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kay</td>\n",
       "      <td>mann-k</td>\n",
       "      <td>691.0</td>\n",
       "      <td>I'm not particularly wild about many of the ch...</td>\n",
       "      <td>Message-ID: &lt;18356955.1075845935087.JavaMail.e...</td>\n",
       "      <td>18356955.1075845935087.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mann, Kay</td>\n",
       "      <td>mann-k</td>\n",
       "      <td>191.0</td>\n",
       "      <td>I'm happy to look at another format.  This was...</td>\n",
       "      <td>Message-ID: &lt;7652644.1075861733089.JavaMail.ev...</td>\n",
       "      <td>7652644.1075861733089.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>Sara</td>\n",
       "      <td>shackleton-s</td>\n",
       "      <td>357.0</td>\n",
       "      <td>Do you have a draft?\\n\\n</td>\n",
       "      <td>Message-ID: &lt;26239005.1075844525500.JavaMail.e...</td>\n",
       "      <td>26239005.1075844525500.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>Sara</td>\n",
       "      <td>shackleton-s</td>\n",
       "      <td>5359.0</td>\n",
       "      <td>Jason:\\n\\nHere's my suggestion for item (ix) [...</td>\n",
       "      <td>Message-ID: &lt;13586110.1075844905248.JavaMail.e...</td>\n",
       "      <td>13586110.1075844905248.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>Sara</td>\n",
       "      <td>shackleton-s</td>\n",
       "      <td>72.0</td>\n",
       "      <td>Looks fine to me.  SS\\n\\n</td>\n",
       "      <td>Message-ID: &lt;5237013.1075844518930.JavaMail.ev...</td>\n",
       "      <td>5237013.1075844518930.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>Sara</td>\n",
       "      <td>shackleton-s</td>\n",
       "      <td>193.0</td>\n",
       "      <td>I spoke with Dale Neuner (confirm desk) a few ...</td>\n",
       "      <td>Message-ID: &lt;22890009.1075844521680.JavaMail.e...</td>\n",
       "      <td>22890009.1075844521680.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>Sara</td>\n",
       "      <td>shackleton-s</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>Tana:  attached are the L/C provisions for the...</td>\n",
       "      <td>Message-ID: &lt;29117984.1075844567240.JavaMail.e...</td>\n",
       "      <td>29117984.1075844567240.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Author        Folder    File  \\\n",
       "0            Kay        mann-k   543.0   \n",
       "1            Kay        mann-k    67.0   \n",
       "2            Kay        mann-k  2643.0   \n",
       "3            Kay        mann-k   691.0   \n",
       "4      Mann, Kay        mann-k   191.0   \n",
       "...          ...           ...     ...   \n",
       "19995       Sara  shackleton-s   357.0   \n",
       "19996       Sara  shackleton-s  5359.0   \n",
       "19997       Sara  shackleton-s    72.0   \n",
       "19998       Sara  shackleton-s   193.0   \n",
       "19999       Sara  shackleton-s  2023.0   \n",
       "\n",
       "                                                    Text  \\\n",
       "0      Here is the latest version.  Our meeting is sc...   \n",
       "1                                    Per your request.\\n   \n",
       "2      The word is that giving the notice could put t...   \n",
       "3      I'm not particularly wild about many of the ch...   \n",
       "4      I'm happy to look at another format.  This was...   \n",
       "...                                                  ...   \n",
       "19995                           Do you have a draft?\\n\\n   \n",
       "19996  Jason:\\n\\nHere's my suggestion for item (ix) [...   \n",
       "19997                          Looks fine to me.  SS\\n\\n   \n",
       "19998  I spoke with Dale Neuner (confirm desk) a few ...   \n",
       "19999  Tana:  attached are the L/C provisions for the...   \n",
       "\n",
       "                                                Raw Text  \\\n",
       "0      Message-ID: <22360372.1075845931625.JavaMail.e...   \n",
       "1      Message-ID: <2759744.1075845920112.JavaMail.ev...   \n",
       "2      Message-ID: <30826773.1075846093464.JavaMail.e...   \n",
       "3      Message-ID: <18356955.1075845935087.JavaMail.e...   \n",
       "4      Message-ID: <7652644.1075861733089.JavaMail.ev...   \n",
       "...                                                  ...   \n",
       "19995  Message-ID: <26239005.1075844525500.JavaMail.e...   \n",
       "19996  Message-ID: <13586110.1075844905248.JavaMail.e...   \n",
       "19997  Message-ID: <5237013.1075844518930.JavaMail.ev...   \n",
       "19998  Message-ID: <22890009.1075844521680.JavaMail.e...   \n",
       "19999  Message-ID: <29117984.1075844567240.JavaMail.e...   \n",
       "\n",
       "                    Message ID  \n",
       "0      22360372.1075845931625.  \n",
       "1       2759744.1075845920112.  \n",
       "2      30826773.1075846093464.  \n",
       "3      18356955.1075845935087.  \n",
       "4       7652644.1075861733089.  \n",
       "...                        ...  \n",
       "19995  26239005.1075844525500.  \n",
       "19996  13586110.1075844905248.  \n",
       "19997   5237013.1075844518930.  \n",
       "19998  22890009.1075844521680.  \n",
       "19999  29117984.1075844567240.  \n",
       "\n",
       "[20000 rows x 6 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the number of samples per author here\n",
    "df = uniform_distribution(4000, df)\n",
    "print(df[\"Folder\"].value_counts())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbb7af9",
   "metadata": {},
   "source": [
    "The function uniform_distribution is used to sample the appropriate number of emails from the number of chosen authors. As you can initially see the number of emails per author is unbalanced with the highest being around 8000 and the lowest being around 4000. To ensure equal distribution we random sampled 4000 emails from each of the 5 authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "facbcb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(text):\n",
    "    #Remove Punctuation Marks\n",
    "    text = text.lower()\n",
    "    nopunct = \"\"\n",
    "    clean_final = []\n",
    "    for char in text:\n",
    "        if re.match(r\"\\w\", char) or re.match(r\" \", char):\n",
    "            nopunct += char\n",
    "        elif re.match(r\" \", char):\n",
    "            nopunct += char\n",
    "        else:\n",
    "            nopunct += \" \"\n",
    "    for word in nopunct.split():\n",
    "        if not word in stop_words:\n",
    "            clean_final.append(word)\n",
    "    nopunct=\" \".join(clean_final)\n",
    "    return nopunct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07469cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        latest version meeting scheduled tomorrow 300 ...\n",
       "1                                              per request\n",
       "2        word giving notice could put equipment balance...\n",
       "3        particularly wild many changes proposed ge see...\n",
       "4          happy look another format one michelle gave kay\n",
       "                               ...                        \n",
       "19995                                                draft\n",
       "19996    jason suggestion item ix additional event defa...\n",
       "19997                                        looks fine ss\n",
       "19998    spoke dale neuner confirm desk minutes ago see...\n",
       "19999    tana attached l c provisions deutsche bank mas...\n",
       "Name: Text, Length: 20000, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Process all the text in each row in the df dataset\n",
    "proccessed_text = df[\"Text\"].apply(lambda row: text_process(row))\n",
    "proccessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c1111251",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Processed Text\"] = proccessed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043e47ac",
   "metadata": {},
   "source": [
    "Here we are doing text preprocessing - remove all non alphanumeric characters, and removing the stop words, and adding that as a column to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "525bb9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = df[\"Processed Text\"].apply(lambda row: word_tokenize(row))\n",
    "df[\"Tokens\"] = tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e059fff1",
   "metadata": {},
   "source": [
    "Converting the processed text to tokens which will then be used to find the vector in the Glove pretrained vector model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7893d0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"Folder\"]\n",
    "labelencoder = LabelEncoder()\n",
    "y = labelencoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c8629b",
   "metadata": {},
   "source": [
    "Here we are label encoding our target variable Y, which in this case is the folder name or author name of the sent email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cfd59d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(list_of_docs, model):\n",
    "\n",
    "    features = []\n",
    "\n",
    "    for tokens in list_of_docs:\n",
    "        zero_vector = np.zeros(model.vector_size)\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in model.wv:\n",
    "                try:\n",
    "                    vectors.append(model.wv[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            avg_vec = vectors.mean(axis=0)\n",
    "            features.append(avg_vec)\n",
    "        else:\n",
    "            features.append(zero_vector)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8077c5c",
   "metadata": {},
   "source": [
    "This function takes input of a list of tokens, a trained word2vec model. The function then goes through all the tokens and finds the appropriate word2vec vector and appends the found vector to a list called vectors. To represent the all the word vectors present in the list vectors, we take the average of the vectors and return that to represent the list of tokens passed into the function. If none of the tokens passed in were found in the glove word dictionary a zero vector is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19706273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1\n",
      "Training Accuracy - 0.9705555555555555\n",
      "Testing Accuracy - 0.7585\n",
      "Round 2\n",
      "Training Accuracy - 0.9707777777777777\n",
      "Testing Accuracy - 0.7265\n",
      "Round 3\n",
      "Training Accuracy - 0.9702222222222222\n",
      "Testing Accuracy - 0.729\n",
      "Round 4\n",
      "Training Accuracy - 0.9706111111111111\n",
      "Testing Accuracy - 0.7485\n",
      "Round 5\n",
      "Training Accuracy - 0.9713888888888889\n",
      "Testing Accuracy - 0.748\n",
      "Round 6\n",
      "Training Accuracy - 0.9706666666666667\n",
      "Testing Accuracy - 0.723\n",
      "Round 7\n",
      "Training Accuracy - 0.9705555555555555\n",
      "Testing Accuracy - 0.7515\n",
      "Round 8\n",
      "Training Accuracy - 0.9704444444444444\n",
      "Testing Accuracy - 0.7565\n",
      "Round 9\n",
      "Training Accuracy - 0.9706111111111111\n",
      "Testing Accuracy - 0.7645\n",
      "Round 10\n",
      "Training Accuracy - 0.9705555555555555\n",
      "Testing Accuracy - 0.745\n",
      "Average Training Accuracy\n",
      "0.9706388888888888\n",
      "Average Testing Accuracy\n",
      "0.7451\n"
     ]
    }
   ],
   "source": [
    "X = df[\"Tokens\"].to_numpy()\n",
    "kf = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "i = 1\n",
    "clf = RandomForestClassifier()\n",
    "training_accuracy = []\n",
    "testing_accuracy = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train = X[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_train = y[train_index]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    tokenized_docs = X_train\n",
    "    model = Word2Vec(sentences=tokenized_docs, vector_size=100, workers=1, seed=42)\n",
    "    vectorized_docs = vectorize(tokenized_docs, model=model)\n",
    "    \n",
    "    test_tokenized_docs = X_test\n",
    "    test_vectorized_docs = vectorize(test_tokenized_docs, model=model)\n",
    "    \n",
    "    clf.fit(vectorized_docs, y_train)\n",
    "    training_accuracy.append(clf.score(vectorized_docs, y_train))\n",
    "    testing_accuracy.append(clf.score(test_vectorized_docs, y_test))\n",
    "    print(\"Round {}\".format(i))\n",
    "    print(\"Training Accuracy - {}\".format(training_accuracy[i-1]))\n",
    "    print(\"Testing Accuracy - {}\".format(testing_accuracy[i-1]))\n",
    "    i = i + 1\n",
    "print(\"Average Training Accuracy\")\n",
    "print(sum(training_accuracy)/len(training_accuracy))\n",
    "print(\"Average Testing Accuracy\")\n",
    "print(sum(testing_accuracy)/len(testing_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed04cb1",
   "metadata": {},
   "source": [
    "Here we are taking the Tokens column and do 10 fold cross validation with word2vec and RandomForests.\n",
    "The unique training data tokens are found and used to train the word2vec model.\n",
    "We then use the vectorizer function to vectorize all the tokens per email, and get one 100d vector representing the entire email. \n",
    "We then use the RandomForest to get our training and testing accuracies.\n",
    "Average Training Accuracy\n",
    "0.9704944444444445\n",
    "Average Testing Accuracy\n",
    "0.74885"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "40215833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1\n",
      "Training Accuracy - 0.7410555555555556\n",
      "Testing Accuracy - 0.7425\n",
      "Round 2\n",
      "Training Accuracy - 0.7416666666666667\n",
      "Testing Accuracy - 0.7325\n",
      "Round 3\n",
      "Training Accuracy - 0.7446666666666667\n",
      "Testing Accuracy - 0.7195\n",
      "Round 4\n",
      "Training Accuracy - 0.7477222222222222\n",
      "Testing Accuracy - 0.739\n",
      "Round 5\n",
      "Training Accuracy - 0.7498333333333334\n",
      "Testing Accuracy - 0.7345\n",
      "Round 6\n",
      "Training Accuracy - 0.7423333333333333\n",
      "Testing Accuracy - 0.7295\n",
      "Round 7\n",
      "Training Accuracy - 0.7466111111111111\n",
      "Testing Accuracy - 0.7425\n",
      "Round 8\n",
      "Training Accuracy - 0.7365555555555555\n",
      "Testing Accuracy - 0.74\n",
      "Round 9\n",
      "Training Accuracy - 0.7495\n",
      "Testing Accuracy - 0.7375\n",
      "Round 10\n",
      "Training Accuracy - 0.7376666666666667\n",
      "Testing Accuracy - 0.735\n",
      "Average Training Accuracy\n",
      "0.7437611111111111\n",
      "Average Testing Accuracy\n",
      "0.73525\n"
     ]
    }
   ],
   "source": [
    "X = df[\"Tokens\"].to_numpy()\n",
    "kf = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "i = 1\n",
    "clf = LogisticRegression(n_jobs=1, C=1e5, max_iter=100000)\n",
    "training_accuracy = []\n",
    "testing_accuracy = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train = X[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_train = y[train_index]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    tokenized_docs = X_train\n",
    "    model = Word2Vec(sentences=tokenized_docs, vector_size=100, workers=1, seed=42)\n",
    "    vectorized_docs = vectorize(tokenized_docs, model=model)\n",
    "    \n",
    "    test_tokenized_docs = X_test\n",
    "    test_vectorized_docs = vectorize(test_tokenized_docs, model=model)\n",
    "    \n",
    "    clf.fit(vectorized_docs, y_train)\n",
    "    training_accuracy.append(clf.score(vectorized_docs, y_train))\n",
    "    testing_accuracy.append(clf.score(test_vectorized_docs, y_test))\n",
    "    print(\"Round {}\".format(i))\n",
    "    print(\"Training Accuracy - {}\".format(training_accuracy[i-1]))\n",
    "    print(\"Testing Accuracy - {}\".format(testing_accuracy[i-1]))\n",
    "    i = i + 1\n",
    "print(\"Average Training Accuracy\")\n",
    "print(sum(training_accuracy)/len(training_accuracy))\n",
    "print(\"Average Testing Accuracy\")\n",
    "print(sum(testing_accuracy)/len(testing_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001e476a",
   "metadata": {},
   "source": [
    "Here we are taking the Tokens column and do 10 fold cross validation with word2vec and LogisticRegression.\n",
    "The unique training data tokens are found and used to train the word2vec model.\n",
    "We then use the vectorizer function to vectorize all the tokens per email, and get one 100d vector representing the entire email. \n",
    "We then use the LogisticRegression to get our training and testing accuracies.\n",
    "Average Training Accuracy\n",
    "0.7484777777777778\n",
    "Average Testing Accuracy\n",
    "0.7414"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e326da9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy\n",
      "0.8923880597014925\n",
      "Testing Accuracy\n",
      "0.7621212121212121\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, shuffle=True, stratify=y)\n",
    "\n",
    "tokenized_docs = X_train\n",
    "model = Word2Vec(sentences=tokenized_docs, vector_size=100, workers=1, seed=42)\n",
    "vectorized_docs = vectorize(tokenized_docs, model=model)\n",
    "    \n",
    "test_tokenized_docs = X_test\n",
    "test_vectorized_docs = vectorize(test_tokenized_docs, model=model)\n",
    "\n",
    "parameters = {\"C\": [1.0, 10], \"gamma\": [1, 'auto', 'scale']}\n",
    "\n",
    "\n",
    "model_clf = GridSearchCV(SVC(kernel='rbf'), parameters, cv=10, n_jobs=-1).fit(vectorized_docs, y_train)\n",
    "training_accuracy = model_clf.score(vectorized_docs, y_train)\n",
    "testing_accuracy = model_clf.score(test_vectorized_docs, y_test)\n",
    "print(\"Training Accuracy\")\n",
    "print(training_accuracy)\n",
    "print(\"Testing Accuracy\")\n",
    "print(testing_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b49758",
   "metadata": {},
   "source": [
    "Here we are using train test split to first intially find the best parameters for Support Vector Machine. With the best parameters using train test split we are getting Training Accuracy\n",
    "0.8880597014925373\n",
    "Testing Accuracy\n",
    "0.7692424242424243"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "648f17aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'gamma': 1}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4607bdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1\n",
      "Training Accuracy - 0.9046111111111111\n",
      "Testing Accuracy - 0.769\n",
      "Round 2\n",
      "Training Accuracy - 0.9072777777777777\n",
      "Testing Accuracy - 0.758\n",
      "Round 3\n",
      "Training Accuracy - 0.9058888888888889\n",
      "Testing Accuracy - 0.7425\n",
      "Round 4\n",
      "Training Accuracy - 0.9111666666666667\n",
      "Testing Accuracy - 0.76\n",
      "Round 5\n",
      "Training Accuracy - 0.9097222222222222\n",
      "Testing Accuracy - 0.7505\n",
      "Round 6\n",
      "Training Accuracy - 0.9094444444444445\n",
      "Testing Accuracy - 0.742\n",
      "Round 7\n",
      "Training Accuracy - 0.9058333333333334\n",
      "Testing Accuracy - 0.757\n",
      "Round 8\n",
      "Training Accuracy - 0.9055\n",
      "Testing Accuracy - 0.768\n",
      "Round 9\n",
      "Training Accuracy - 0.9057777777777778\n",
      "Testing Accuracy - 0.763\n",
      "Round 10\n",
      "Training Accuracy - 0.9101666666666667\n",
      "Testing Accuracy - 0.747\n",
      "Average Training Accuracy\n",
      "0.907538888888889\n",
      "Average Testing Accuracy\n",
      "0.7556999999999999\n"
     ]
    }
   ],
   "source": [
    "X = df[\"Tokens\"].to_numpy()\n",
    "kf = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "i = 1\n",
    "clf = model_clf # Referencing the previous Support Vector Classifier with the best parameters\n",
    "training_accuracy = []\n",
    "testing_accuracy = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train = X[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_train = y[train_index]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    tokenized_docs = X_train\n",
    "    model = Word2Vec(sentences=tokenized_docs, vector_size=100, workers=1, seed=42)\n",
    "    vectorized_docs = vectorize(tokenized_docs, model=model)\n",
    "    \n",
    "    test_tokenized_docs = X_test\n",
    "    test_vectorized_docs = vectorize(test_tokenized_docs, model=model)\n",
    "    \n",
    "    clf.fit(vectorized_docs, y_train)\n",
    "    training_accuracy.append(clf.score(vectorized_docs, y_train))\n",
    "    testing_accuracy.append(clf.score(test_vectorized_docs, y_test))\n",
    "    print(\"Round {}\".format(i))\n",
    "    print(\"Training Accuracy - {}\".format(training_accuracy[i-1]))\n",
    "    print(\"Testing Accuracy - {}\".format(testing_accuracy[i-1]))\n",
    "    i = i + 1\n",
    "print(\"Average Training Accuracy\")\n",
    "print(sum(training_accuracy)/len(training_accuracy))\n",
    "print(\"Average Testing Accuracy\")\n",
    "print(sum(testing_accuracy)/len(testing_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34da51b8",
   "metadata": {},
   "source": [
    "Using the best parameters for SVM, we are doing 10 fold cross validation. \n",
    "Average Training Accuracy\n",
    "0.907538888888889\n",
    "Average Testing Accuracy\n",
    "0.7556999999999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "119f7ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec_training_acc_rf = 0.9704944444444445 * 100\n",
    "wordvec_testing_acc_rf = 0.74885 * 100\n",
    "wordvec_training_acc_lr = 0.7484777777777778 * 100\n",
    "wordvec_testing_acc_lr = 0.7414 * 100\n",
    "wordvec_training_acc_svm = 0.907538888888889 * 100\n",
    "wordvec_acc_svm = 0.7556999999999999 * 100\n",
    "training_acc = [wordvec_training_acc_rf, wordvec_training_acc_lr, wordvec_training_acc_svm]\n",
    "testing_acc = [wordvec_testing_acc_rf, wordvec_testing_acc_lr, wordvec_acc_svm]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3b09bffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = [\"Random Forests\", \"Logistic Regression\", \"Support Vector Machine\"]\n",
    "x = np.arange(len(labels))\n",
    "length = np.arange(len(labels))\n",
    "width = 0.2\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, training_acc, width, label='Training Accuracies')\n",
    "rects2 = ax.bar(x + width/2, testing_acc, width, label='Testing Accuracies')\n",
    "ax.set_ylabel('Accuracies')\n",
    "ax.set_title('Word2Vec Accuracies')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc977a2",
   "metadata": {},
   "source": [
    "Plotting the average training and testing accuracies after 10 Fold Cross Validation for each all the classifiers used. We can see that Support Vector Machine has the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8ba84f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
