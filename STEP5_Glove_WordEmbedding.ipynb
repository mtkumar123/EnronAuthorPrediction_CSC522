{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fb0fdded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/manoj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from collections import Counter \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddc0706c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Folder</th>\n",
       "      <th>File</th>\n",
       "      <th>Message ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Raw Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vince J</td>\n",
       "      <td>kaminski-v</td>\n",
       "      <td>2191.0</td>\n",
       "      <td>9763257.1075856555112.</td>\n",
       "      <td>Pierre-Philippe,\\n\\nI have contacted Allison B...</td>\n",
       "      <td>Message-ID: &lt;9763257.1075856555112.JavaMail.ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vince J</td>\n",
       "      <td>kaminski-v</td>\n",
       "      <td>1693.0</td>\n",
       "      <td>31194175.1075856543039.</td>\n",
       "      <td>Martin,\\n\\nCan you, please, call Shu and provi...</td>\n",
       "      <td>Message-ID: &lt;31194175.1075856543039.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vince J</td>\n",
       "      <td>kaminski-v</td>\n",
       "      <td>1952.0</td>\n",
       "      <td>4743153.1075856549060.</td>\n",
       "      <td>UPS Tracking numbers\\n</td>\n",
       "      <td>Message-ID: &lt;4743153.1075856549060.JavaMail.ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vince J</td>\n",
       "      <td>kaminski-v</td>\n",
       "      <td>4054.0</td>\n",
       "      <td>8413088.1075856988030.</td>\n",
       "      <td>Steve,\\n\\nThe schedule is fine with me. One co...</td>\n",
       "      <td>Message-ID: &lt;8413088.1075856988030.JavaMail.ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vince J</td>\n",
       "      <td>kaminski-v</td>\n",
       "      <td>1023.0</td>\n",
       "      <td>20177981.1075856527141.</td>\n",
       "      <td>Zimin,\\n\\nCan you interview this guy? Maybe Ta...</td>\n",
       "      <td>Message-ID: &lt;20177981.1075856527141.JavaMail.e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Author      Folder    File               Message ID  \\\n",
       "0  Vince J  kaminski-v  2191.0   9763257.1075856555112.   \n",
       "1  Vince J  kaminski-v  1693.0  31194175.1075856543039.   \n",
       "2  Vince J  kaminski-v  1952.0   4743153.1075856549060.   \n",
       "3  Vince J  kaminski-v  4054.0   8413088.1075856988030.   \n",
       "4  Vince J  kaminski-v  1023.0  20177981.1075856527141.   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Pierre-Philippe,\\n\\nI have contacted Allison B...   \n",
       "1  Martin,\\n\\nCan you, please, call Shu and provi...   \n",
       "2                             UPS Tracking numbers\\n   \n",
       "3  Steve,\\n\\nThe schedule is fine with me. One co...   \n",
       "4  Zimin,\\n\\nCan you interview this guy? Maybe Ta...   \n",
       "\n",
       "                                            Raw Text  \n",
       "0  Message-ID: <9763257.1075856555112.JavaMail.ev...  \n",
       "1  Message-ID: <31194175.1075856543039.JavaMail.e...  \n",
       "2  Message-ID: <4743153.1075856549060.JavaMail.ev...  \n",
       "3  Message-ID: <8413088.1075856988030.JavaMail.ev...  \n",
       "4  Message-ID: <20177981.1075856527141.JavaMail.e...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Put in the file path to the dataset created from extractingauthors.ipynb\n",
    "df = pd.read_csv(\"./enron.csv\")\n",
    "df = df.drop([\"Email Folder\"], axis=1)\n",
    "#We need only the top 20 authors ordered by number of emails found in either the\n",
    "#sent folder or _sent_mail folder\n",
    "\n",
    "#Add top_authors = df.value_counts([\"Folder\"])[:X] for the number of authors required\n",
    "# Change X to 5,10,15 to test with 5, 10, 15 authors\n",
    "top_authors = df.value_counts([\"Folder\"])[:5]\n",
    "df = df.loc[df[\"Folder\"].isin(list(top_authors.index.get_level_values(0)))].drop([\"Unnamed: 0\"], axis=1).reset_index(drop=True)\n",
    "df = df[df[\"Text\"]!=\" \"]\n",
    "df = df[df[\"Text\"]!=\"\\n\"]\n",
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187460b5",
   "metadata": {},
   "source": [
    "Here we are using the enron.csv file created from the noteboook \"STEP2_ExtractingAuthorEmails.ipynb\". This csv contains all the sent emails from 20 authors, and the extracted body of text from each email. \n",
    "You can change the X value too 5,10,15 to test with the corresponding number of authors. Here we are testing with 5 authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fadcc27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mann-k          8167\n",
       "kaminski-v      5926\n",
       "dasovich-j      4805\n",
       "germany-c       4571\n",
       "shackleton-s    4003\n",
       "Name: Folder, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Folder\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbe3313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_distribution(samples_per_author, df):\n",
    "    df3 = pd.DataFrame(columns=[\"Author\", \"Folder\", \"File\", \"Text\", \"Raw Text\"]) \n",
    "    for folder in df[\"Folder\"].value_counts().index:\n",
    "        df3 = df3.append(df[df[\"Folder\"]==folder].sample(n=samples_per_author), ignore_index=True)\n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7d7f2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mann-k          4000\n",
      "kaminski-v      4000\n",
      "dasovich-j      4000\n",
      "germany-c       4000\n",
      "shackleton-s    4000\n",
      "Name: Folder, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Change the number of samples per author here\n",
    "df = uniform_distribution(4000, df)\n",
    "print(df[\"Folder\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57372983",
   "metadata": {},
   "source": [
    "The function uniform_distribution is used to sample the appropriate number of emails from the number of chosen authors. As you can initially see the number of emails per author is unbalanced with the highest being around 8000 and the lowest being around 4000. To ensure equal distribution we random sampled 4000 emails from each of the 5 authors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "facbcb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(text):\n",
    "    #Remove Punctuation Marks\n",
    "    text = text.lower()\n",
    "    nopunct = \"\"\n",
    "    clean_final = []\n",
    "    for char in text:\n",
    "        if re.match(r\"\\w\", char) or re.match(r\" \", char):\n",
    "            nopunct += char\n",
    "        elif re.match(r\" \", char):\n",
    "            nopunct += char\n",
    "        else:\n",
    "            nopunct += \" \"\n",
    "    for word in nopunct.split():\n",
    "        if not word in stop_words:\n",
    "            clean_final.append(word)\n",
    "    nopunct=\" \".join(clean_final)\n",
    "    return nopunct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07469cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        great ge brett rep john ayres initially pages ...\n",
       "1        brian sure understand objection wording guaran...\n",
       "2        mean literally across street yikes making toug...\n",
       "3                                              next monday\n",
       "4                                                 fun file\n",
       "                               ...                        \n",
       "19995    lawyer apea call let know confirm sent apea se...\n",
       "19996    faxing copy statement submitted ubs enron corp...\n",
       "19997    confirmation desk recently received call mike ...\n",
       "19998    susan attached electronic agreements ena tradi...\n",
       "19999                                                 fine\n",
       "Name: Text, Length: 20000, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Process all the text in each row in the df dataset\n",
    "proccessed_text = df[\"Text\"].apply(lambda row: text_process(row))\n",
    "proccessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1111251",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Processed Text\"] = proccessed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987ce03a",
   "metadata": {},
   "source": [
    "Here we are doing text preprocessing - remove all non alphanumeric characters, and removing the stop words, and adding that as a column to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "525bb9cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [great, ge, brett, rep, john, ayres, initially...\n",
       "1        [brian, sure, understand, objection, wording, ...\n",
       "2        [mean, literally, across, street, yikes, makin...\n",
       "3                                           [next, monday]\n",
       "4                                              [fun, file]\n",
       "                               ...                        \n",
       "19995    [lawyer, apea, call, let, know, confirm, sent,...\n",
       "19996    [faxing, copy, statement, submitted, ubs, enro...\n",
       "19997    [confirmation, desk, recently, received, call,...\n",
       "19998    [susan, attached, electronic, agreements, ena,...\n",
       "19999                                               [fine]\n",
       "Name: Tokens, Length: 20000, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text = df[\"Processed Text\"].apply(lambda row: word_tokenize(row))\n",
    "df[\"Tokens\"] = tokenized_text\n",
    "df[\"Tokens\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadffb99",
   "metadata": {},
   "source": [
    "Converting the processed text to tokens which will then be used to find the vector in the Glove pretrained vector model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7893d0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"Folder\"]\n",
    "labelencoder = LabelEncoder()\n",
    "y = labelencoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4163f927",
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_corpus_emails = df[\"Processed Text\"].tolist()\n",
    "entire_corpus_text = \" \".join(entire_corpus_emails)\n",
    "all_words = entire_corpus_text.split()\n",
    "all_unique_words = set(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "319ad9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens 23714\n",
      "Number of unique words 23714\n"
     ]
    }
   ],
   "source": [
    "all_unique_tokens = list(set([a for b in df[\"Tokens\"].tolist() for a in b]))\n",
    "print(\"Number of unique tokens {}\".format(len(all_unique_tokens)))\n",
    "print(\"Number of unique words {}\".format(len(all_unique_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b3c52c",
   "metadata": {},
   "source": [
    "Here we are label encoding our target variable Y, which in this case is the folder name or author name of the sent email. \n",
    "\n",
    "Then we are creating a set that contains all the unique words seen in the entire \"Processed Text\" column of the enron.csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d8a33eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_glove_vector_dictionary(unique_tokens, f):\n",
    "    glove_word_dict = {}\n",
    "    for line in tqdm(f):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if word in unique_tokens:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            glove_word_dict[word] = coefs\n",
    "    return glove_word_dict    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cc89d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1917495it [28:33, 1118.91it/s]\n"
     ]
    }
   ],
   "source": [
    "GLOVE_DATASET_PATH = \"../glove.42B.300d.txt\"\n",
    "f = open(GLOVE_DATASET_PATH)\n",
    "glove_word_dict = create_glove_vector_dictionary(all_unique_tokens, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa281b34",
   "metadata": {},
   "source": [
    "Here we are opening up the pretrained glove vector model - Common Crawl with 42 billion tokens and 300 dimensional vector, and using the function create_glove_vector_dictionary to map each word seen in the email corpus to the appropriate pre-trained vector according to GLoVe. This process takes about 30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72eb2505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words = 23714\n",
      "Number of words found in Glove = 22811\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of unique words = {}\".format(len(all_unique_words)))\n",
    "print(\"Number of words found in Glove = {}\".format(len(glove_word_dict.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff03117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('glove_word_dict.json', 'wb') as fp:\n",
    "#     pickle.dump(glove_word_dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1a80de",
   "metadata": {},
   "source": [
    "Use the above to save the glove_word_dictionary created earlier, to avoid having to repeatedly use the glove.42B.300d.txt file to search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7061a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_vectorizer(tokenized_text, glove_word_dict):\n",
    "    features = []\n",
    "    zero_vector = np.zeros(300)\n",
    "    vectors = []\n",
    "    word_counter = 0\n",
    "    for word in tokenized_text:\n",
    "        if word in glove_word_dict.keys():\n",
    "            vectors.append(glove_word_dict[word])\n",
    "            word_counter += 1\n",
    "    if vectors:\n",
    "        vectors = np.asarray(vectors)\n",
    "        avg_vectors = vectors.mean(axis=0)\n",
    "        return avg_vectors\n",
    "    else:\n",
    "        return zero_vector\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebea168",
   "metadata": {},
   "source": [
    "This function takes input of a list of tokens, and a glove word dictionary where the key is the token and the value is it's 300d vector. The function then goes through all the tokens and finds the appropriate glove vector and appends the found vector to a list called vectors. \n",
    "To represent the all the word vectors present in the list vectors, we take the average of the vectors and return that to represent the list of tokens passed into the function. If none of the tokens passed in were found in the glove word dictionary a zero vector is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "39dd31ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To spend up test times use this function\n",
    "def create_train_glove_dictionary(unique_tokens):\n",
    "    with open('glove_word_dict.json', 'rb') as fp:\n",
    "        og_dict = pickle.load(fp)\n",
    "    train_dict = {}\n",
    "    for word in unique_tokens:\n",
    "        if word in og_dict.keys():\n",
    "            train_dict[word] = og_dict[word]\n",
    "    return train_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "19706273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1\n",
      "X_Train Shape\n",
      "(18000,)\n",
      "X_Test Shape\n",
      "(2000,)\n",
      "Number of unique tokens 22690\n",
      "\n",
      "Number of unique tokens 22690\n",
      "Number of words found in Glove = 19430\n",
      "X_train vectors\n",
      "(18000, 300)\n",
      "X_test vectors\n",
      "(2000, 300)\n",
      "Training Accuracy - 0.9741111111111111\n",
      "Testing Accuracy - 0.804\n",
      "Round 2\n",
      "X_Train Shape\n",
      "(18000,)\n",
      "X_Test Shape\n",
      "(2000,)\n",
      "Number of unique tokens 22776\n",
      "\n",
      "Number of unique tokens 22776\n",
      "Number of words found in Glove = 19528\n",
      "X_train vectors\n",
      "(18000, 300)\n",
      "X_test vectors\n",
      "(2000, 300)\n",
      "Training Accuracy - 0.9747222222222223\n",
      "Testing Accuracy - 0.7775\n",
      "Round 3\n",
      "X_Train Shape\n",
      "(18000,)\n",
      "X_Test Shape\n",
      "(2000,)\n",
      "Number of unique tokens 22859\n",
      "\n",
      "Number of unique tokens 22859\n",
      "Number of words found in Glove = 19598\n",
      "X_train vectors\n",
      "(18000, 300)\n",
      "X_test vectors\n",
      "(2000, 300)\n",
      "Training Accuracy - 0.9746666666666667\n",
      "Testing Accuracy - 0.795\n",
      "Round 4\n",
      "X_Train Shape\n",
      "(18000,)\n",
      "X_Test Shape\n",
      "(2000,)\n",
      "Number of unique tokens 22820\n",
      "\n",
      "Number of unique tokens 22820\n",
      "Number of words found in Glove = 19538\n",
      "X_train vectors\n",
      "(18000, 300)\n",
      "X_test vectors\n",
      "(2000, 300)\n",
      "Training Accuracy - 0.975\n",
      "Testing Accuracy - 0.7825\n",
      "Round 5\n",
      "X_Train Shape\n",
      "(18000,)\n",
      "X_Test Shape\n",
      "(2000,)\n",
      "Number of unique tokens 22730\n",
      "\n",
      "Number of unique tokens 22730\n",
      "Number of words found in Glove = 19448\n",
      "X_train vectors\n",
      "(18000, 300)\n",
      "X_test vectors\n",
      "(2000, 300)\n",
      "Training Accuracy - 0.9745\n",
      "Testing Accuracy - 0.777\n",
      "Round 6\n",
      "X_Train Shape\n",
      "(18000,)\n",
      "X_Test Shape\n",
      "(2000,)\n",
      "Number of unique tokens 22594\n",
      "\n",
      "Number of unique tokens 22594\n",
      "Number of words found in Glove = 19264\n",
      "X_train vectors\n",
      "(18000, 300)\n",
      "X_test vectors\n",
      "(2000, 300)\n",
      "Training Accuracy - 0.9749444444444444\n",
      "Testing Accuracy - 0.7915\n",
      "Round 7\n",
      "X_Train Shape\n",
      "(18000,)\n",
      "X_Test Shape\n",
      "(2000,)\n",
      "Number of unique tokens 22903\n",
      "\n",
      "Number of unique tokens 22903\n",
      "Number of words found in Glove = 19577\n",
      "X_train vectors\n",
      "(18000, 300)\n",
      "X_test vectors\n",
      "(2000, 300)\n",
      "Training Accuracy - 0.9752222222222222\n",
      "Testing Accuracy - 0.777\n",
      "Round 8\n",
      "X_Train Shape\n",
      "(18000,)\n",
      "X_Test Shape\n",
      "(2000,)\n",
      "Number of unique tokens 22755\n",
      "\n",
      "Number of unique tokens 22755\n",
      "Number of words found in Glove = 19476\n",
      "X_train vectors\n",
      "(18000, 300)\n",
      "X_test vectors\n",
      "(2000, 300)\n",
      "Training Accuracy - 0.9746666666666667\n",
      "Testing Accuracy - 0.7805\n",
      "Round 9\n",
      "X_Train Shape\n",
      "(18000,)\n",
      "X_Test Shape\n",
      "(2000,)\n",
      "Number of unique tokens 22765\n",
      "\n",
      "Number of unique tokens 22765\n",
      "Number of words found in Glove = 19501\n",
      "X_train vectors\n",
      "(18000, 300)\n",
      "X_test vectors\n",
      "(2000, 300)\n",
      "Training Accuracy - 0.9753888888888889\n",
      "Testing Accuracy - 0.784\n",
      "Round 10\n",
      "X_Train Shape\n",
      "(18000,)\n",
      "X_Test Shape\n",
      "(2000,)\n",
      "Number of unique tokens 22942\n",
      "\n",
      "Number of unique tokens 22942\n",
      "Number of words found in Glove = 19588\n",
      "X_train vectors\n",
      "(18000, 300)\n",
      "X_test vectors\n",
      "(2000, 300)\n",
      "Training Accuracy - 0.9754444444444444\n",
      "Testing Accuracy - 0.781\n",
      "Average Training Accuracy\n",
      "0.9748666666666669\n",
      "Average Testing Accuracy\n",
      "0.7849999999999999\n"
     ]
    }
   ],
   "source": [
    "X = df[\"Tokens\"]\n",
    "kf = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "i = 1\n",
    "clf = RandomForestClassifier()\n",
    "training_accuracy = []\n",
    "testing_accuracy = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train = X[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_train = y[train_index]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    print(\"Round {}\".format(i))\n",
    "    print(\"X_Train Shape\")\n",
    "    print(X_train.shape)\n",
    "    print(\"X_Test Shape\")\n",
    "    print(X_test.shape)\n",
    "    \n",
    "    all_unique_tokens = list(set([a for b in X_train.tolist() for a in b]))\n",
    "    print(\"Number of unique tokens {}\".format(len(all_unique_tokens)))\n",
    "    print()\n",
    "    \n",
    "    #This will take super long, so instead create the smaller training dictionary from the og_dict\n",
    "#     GLOVE_DATASET_PATH = \"./glove.42B.300d.txt\"\n",
    "#     f = open(GLOVE_DATASET_PATH)\n",
    "#     glove_word_dict = create_glove_vector_dictionary(all_unique_tokens, f)\n",
    "#     f.close()\n",
    "\n",
    "    glove_word_dict = create_train_glove_dictionary(all_unique_tokens)\n",
    "    \n",
    "    print(\"Number of unique tokens {}\".format(len(all_unique_tokens)))\n",
    "    print(\"Number of words found in Glove = {}\".format(len(glove_word_dict.keys())))\n",
    "    \n",
    "    X_train_vectors = X_train.apply(lambda row: glove_vectorizer(row, glove_word_dict))\n",
    "    X_test_vectors = X_test.apply(lambda row: glove_vectorizer(row, glove_word_dict))\n",
    "    \n",
    "    X_train_vectors = np.array(X_train_vectors.tolist())\n",
    "    X_test_vectors = np.array(X_test_vectors.tolist())\n",
    "    \n",
    "    print(\"X_train vectors\")\n",
    "    print(X_train_vectors.shape)\n",
    "    print(\"X_test vectors\")\n",
    "    print(X_test_vectors.shape)\n",
    "    \n",
    "    clf.fit(X_train_vectors, y_train)\n",
    "    training_accuracy.append(clf.score(X_train_vectors, y_train))\n",
    "    testing_accuracy.append(clf.score(X_test_vectors, y_test))\n",
    "    print(\"Training Accuracy - {}\".format(training_accuracy[i-1]))\n",
    "    print(\"Testing Accuracy - {}\".format(testing_accuracy[i-1]))\n",
    "    i = i + 1\n",
    "print(\"Average Training Accuracy\")\n",
    "print(sum(training_accuracy)/len(training_accuracy))\n",
    "print(\"Average Testing Accuracy\")\n",
    "print(sum(testing_accuracy)/len(testing_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e520a0",
   "metadata": {},
   "source": [
    "Here we are taking the Tokens column and do 10 fold cross validation with Glove Vector and RandomForests.\n",
    "The unique training data tokens are found and then passed into create_train_glove_dictionary to create a subset of the glove_word_dictionary that was created using all the unique tokens from the entire dataset. This is done so that the training data and testing data are kept separate, but also to be able to have a faster runtime, as it would take a lot of time to do 10 fold cross validation using the Glove Vector file. Using the glove_word_dictionary created originally we create a smaller dictionary that consists of only the tokens seen in the training data. \n",
    "We then use the glove vectorizer to vectorize the tokens per email, and get one 300d vector representing the entire email. \n",
    "We then use the RandomForest to get our training and testing accuracies.\n",
    "Average Training Accuracy\n",
    "0.9748666666666669\n",
    "Average Testing Accuracy\n",
    "0.7849999999999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "40215833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1\n",
      "X_Train Shape\n",
      "(18000,)\n",
      "X_Test Shape\n",
      "(2000,)\n",
      "Number of unique tokens 22563\n",
      "\n",
      "Number of unique tokens 22563\n",
      "Number of words found in Glove = 20495\n",
      "X_train vectors\n",
      "(18000, 300)\n",
      "X_test vectors\n",
      "(2000, 300)\n",
      "Training Accuracy - 0.7887222222222222\n",
      "Testing Accuracy - 0.76\n",
      "Round 2\n",
      "X_Train Shape\n",
      "(18000,)\n",
      "X_Test Shape\n",
      "(2000,)\n",
      "Number of unique tokens 22564\n",
      "\n",
      "Number of unique tokens 22564\n",
      "Number of words found in Glove = 20506\n",
      "X_train vectors\n",
      "(18000, 300)\n",
      "X_test vectors\n",
      "(2000, 300)\n",
      "Training Accuracy - 0.7862777777777777\n",
      "Testing Accuracy - 0.7645\n",
      "Round 3\n",
      "X_Train Shape\n",
      "(18000,)\n",
      "X_Test Shape\n",
      "(2000,)\n",
      "Number of unique tokens 22444\n",
      "\n",
      "Number of unique tokens 22444\n",
      "Number of words found in Glove = 20386\n",
      "X_train vectors\n",
      "(18000, 300)\n",
      "X_test vectors\n",
      "(2000, 300)\n",
      "Training Accuracy - 0.7887777777777778\n",
      "Testing Accuracy - 0.763\n",
      "Round 4\n",
      "X_Train Shape\n",
      "(18000,)\n",
      "X_Test Shape\n",
      "(2000,)\n",
      "Number of unique tokens 22504\n",
      "\n",
      "Number of unique tokens 22504\n",
      "Number of words found in Glove = 20468\n",
      "X_train vectors\n",
      "(18000, 300)\n",
      "X_test vectors\n",
      "(2000, 300)\n",
      "Training Accuracy - 0.7897777777777778\n",
      "Testing Accuracy - 0.7565\n",
      "Round 5\n",
      "X_Train Shape\n",
      "(18000,)\n",
      "X_Test Shape\n",
      "(2000,)\n",
      "Number of unique tokens 22389\n",
      "\n",
      "Number of unique tokens 22389\n",
      "Number of words found in Glove = 20335\n",
      "X_train vectors\n",
      "(18000, 300)\n",
      "X_test vectors\n",
      "(2000, 300)\n",
      "Training Accuracy - 0.7873888888888889\n",
      "Testing Accuracy - 0.7665\n",
      "Round 6\n",
      "X_Train Shape\n",
      "(18000,)\n",
      "X_Test Shape\n",
      "(2000,)\n",
      "Number of unique tokens 22590\n",
      "\n",
      "Number of unique tokens 22590\n",
      "Number of words found in Glove = 20518\n",
      "X_train vectors\n",
      "(18000, 300)\n",
      "X_test vectors\n",
      "(2000, 300)\n",
      "Training Accuracy - 0.7863333333333333\n",
      "Testing Accuracy - 0.777\n",
      "Round 7\n",
      "X_Train Shape\n",
      "(18000,)\n",
      "X_Test Shape\n",
      "(2000,)\n",
      "Number of unique tokens 22471\n",
      "\n",
      "Number of unique tokens 22471\n",
      "Number of words found in Glove = 20388\n",
      "X_train vectors\n",
      "(18000, 300)\n",
      "X_test vectors\n",
      "(2000, 300)\n",
      "Training Accuracy - 0.7866111111111111\n",
      "Testing Accuracy - 0.7675\n",
      "Round 8\n",
      "X_Train Shape\n",
      "(18000,)\n",
      "X_Test Shape\n",
      "(2000,)\n",
      "Number of unique tokens 22465\n",
      "\n",
      "Number of unique tokens 22465\n",
      "Number of words found in Glove = 20370\n",
      "X_train vectors\n",
      "(18000, 300)\n",
      "X_test vectors\n",
      "(2000, 300)\n",
      "Training Accuracy - 0.7871666666666667\n",
      "Testing Accuracy - 0.7605\n",
      "Round 9\n",
      "X_Train Shape\n",
      "(18000,)\n",
      "X_Test Shape\n",
      "(2000,)\n",
      "Number of unique tokens 22455\n",
      "\n",
      "Number of unique tokens 22455\n",
      "Number of words found in Glove = 20388\n",
      "X_train vectors\n",
      "(18000, 300)\n",
      "X_test vectors\n",
      "(2000, 300)\n",
      "Training Accuracy - 0.7871666666666667\n",
      "Testing Accuracy - 0.773\n",
      "Round 10\n",
      "X_Train Shape\n",
      "(18000,)\n",
      "X_Test Shape\n",
      "(2000,)\n",
      "Number of unique tokens 22609\n",
      "\n",
      "Number of unique tokens 22609\n",
      "Number of words found in Glove = 20535\n",
      "X_train vectors\n",
      "(18000, 300)\n",
      "X_test vectors\n",
      "(2000, 300)\n",
      "Training Accuracy - 0.7857777777777778\n",
      "Testing Accuracy - 0.767\n",
      "Average Training Accuracy\n",
      "0.7874000000000001\n",
      "Average Testing Accuracy\n",
      "0.7655500000000001\n"
     ]
    }
   ],
   "source": [
    "X = df[\"Tokens\"]\n",
    "kf = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "i = 1\n",
    "clf = LogisticRegression(n_jobs=1, C=1e5, max_iter=100000)\n",
    "training_accuracy = []\n",
    "testing_accuracy = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train = X[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_train = y[train_index]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    print(\"Round {}\".format(i))\n",
    "    print(\"X_Train Shape\")\n",
    "    print(X_train.shape)\n",
    "    print(\"X_Test Shape\")\n",
    "    print(X_test.shape)\n",
    "    \n",
    "    all_unique_tokens = list(set([a for b in X_train.tolist() for a in b]))\n",
    "    print(\"Number of unique tokens {}\".format(len(all_unique_tokens)))\n",
    "    print()\n",
    "    \n",
    "    #This will take super long, so instead create the smaller training dictionary from the og_dict\n",
    "#     GLOVE_DATASET_PATH = \"./glove.42B.300d.txt\"\n",
    "#     f = open(GLOVE_DATASET_PATH)\n",
    "#     glove_word_dict = create_glove_vector_dictionary(all_unique_tokens, f)\n",
    "#     f.close()\n",
    "\n",
    "    glove_word_dict = create_train_glove_dictionary(all_unique_tokens)\n",
    "    \n",
    "    print(\"Number of unique tokens {}\".format(len(all_unique_tokens)))\n",
    "    print(\"Number of words found in Glove = {}\".format(len(glove_word_dict.keys())))\n",
    "    \n",
    "    X_train_vectors = X_train.apply(lambda row: glove_vectorizer(row, glove_word_dict))\n",
    "    X_test_vectors = X_test.apply(lambda row: glove_vectorizer(row, glove_word_dict))\n",
    "    \n",
    "    X_train_vectors = np.array(X_train_vectors.tolist())\n",
    "    X_test_vectors = np.array(X_test_vectors.tolist())\n",
    "    \n",
    "    print(\"X_train vectors\")\n",
    "    print(X_train_vectors.shape)\n",
    "    print(\"X_test vectors\")\n",
    "    print(X_test_vectors.shape)\n",
    "    \n",
    "    \n",
    "    clf.fit(X_train_vectors, y_train)\n",
    "    training_accuracy.append(clf.score(X_train_vectors, y_train))\n",
    "    testing_accuracy.append(clf.score(X_test_vectors, y_test))\n",
    "    print(\"Training Accuracy - {}\".format(training_accuracy[i-1]))\n",
    "    print(\"Testing Accuracy - {}\".format(testing_accuracy[i-1]))\n",
    "    i = i + 1\n",
    "print(\"Average Training Accuracy\")\n",
    "print(sum(training_accuracy)/len(training_accuracy))\n",
    "print(\"Average Testing Accuracy\")\n",
    "print(sum(testing_accuracy)/len(testing_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98182695",
   "metadata": {},
   "source": [
    "Here we use Logistic Regression to get our training and testing accuracies.\n",
    "Average Training Accuracy\n",
    "0.7874000000000001\n",
    "Average Testing Accuracy\n",
    "0.7655500000000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e326da9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens 20215\n",
      "\n",
      "Number of unique tokens 20215\n",
      "Number of words found in Glove = 17515\n",
      "X_train vectors\n",
      "(13400, 300)\n",
      "X_test vectors\n",
      "(6600, 300)\n",
      "Training Accuracy\n",
      "0.9305970149253732\n",
      "Testing Accuracy\n",
      "0.825\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, shuffle=True, stratify=y)\n",
    "\n",
    "\n",
    "all_unique_tokens = list(set([a for b in X_train.tolist() for a in b]))\n",
    "print(\"Number of unique tokens {}\".format(len(all_unique_tokens)))\n",
    "print()\n",
    "\n",
    "glove_word_dict = create_train_glove_dictionary(all_unique_tokens)\n",
    "    \n",
    "print(\"Number of unique tokens {}\".format(len(all_unique_tokens)))\n",
    "print(\"Number of words found in Glove = {}\".format(len(glove_word_dict.keys())))\n",
    "    \n",
    "X_train_vectors = X_train.apply(lambda row: glove_vectorizer(row, glove_word_dict))\n",
    "X_test_vectors = X_test.apply(lambda row: glove_vectorizer(row, glove_word_dict))\n",
    "\n",
    "X_train_vectors = np.array(X_train_vectors.tolist())\n",
    "X_test_vectors = np.array(X_test_vectors.tolist())\n",
    "\n",
    "print(\"X_train vectors\")\n",
    "print(X_train_vectors.shape)\n",
    "print(\"X_test vectors\")\n",
    "print(X_test_vectors.shape)\n",
    "\n",
    "parameters = {\"C\": [1.0, 10], \"gamma\": [1, 'auto', 'scale']}\n",
    "\n",
    "\n",
    "model = GridSearchCV(SVC(kernel='rbf'), parameters, cv=10, n_jobs=-1).fit(X_train_vectors, y_train)\n",
    "training_accuracy = model.score(X_train_vectors, y_train)\n",
    "testing_accuracy = model.score(X_test_vectors, y_test)\n",
    "print(\"Training Accuracy\")\n",
    "print(training_accuracy)\n",
    "print(\"Testing Accuracy\")\n",
    "print(testing_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f18a41",
   "metadata": {},
   "source": [
    "Here we are using train test split to first intially find the best parameters for Support Vector Machine. With the best parameters using train test split we are getting\n",
    "Training Accuracy\n",
    "0.9305970149253732\n",
    "Testing Accuracy\n",
    "0.825"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "50ebaf5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'gamma': 'scale'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bde9ed44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1\n",
      "X_Train Shape\n",
      "(18000,)\n",
      "X_Test Shape\n",
      "(2000,)\n",
      "Number of unique tokens 22690\n",
      "\n",
      "Number of unique tokens 22690\n",
      "Number of words found in Glove = 19430\n",
      "X_train vectors\n",
      "(18000, 300)\n",
      "X_test vectors\n",
      "(2000, 300)\n",
      "Training Accuracy - 0.9276666666666666\n",
      "Testing Accuracy - 0.846\n",
      "Round 2\n",
      "X_Train Shape\n",
      "(18000,)\n",
      "X_Test Shape\n",
      "(2000,)\n",
      "Number of unique tokens 22776\n",
      "\n",
      "Number of unique tokens 22776\n",
      "Number of words found in Glove = 19528\n",
      "X_train vectors\n",
      "(18000, 300)\n",
      "X_test vectors\n",
      "(2000, 300)\n",
      "Training Accuracy - 0.9293888888888889\n",
      "Testing Accuracy - 0.825\n",
      "Round 3\n",
      "X_Train Shape\n",
      "(18000,)\n",
      "X_Test Shape\n",
      "(2000,)\n",
      "Number of unique tokens 22859\n",
      "\n",
      "Number of unique tokens 22859\n",
      "Number of words found in Glove = 19598\n",
      "X_train vectors\n",
      "(18000, 300)\n",
      "X_test vectors\n",
      "(2000, 300)\n",
      "Training Accuracy - 0.9294444444444444\n",
      "Testing Accuracy - 0.84\n",
      "Round 4\n",
      "X_Train Shape\n",
      "(18000,)\n",
      "X_Test Shape\n",
      "(2000,)\n",
      "Number of unique tokens 22820\n",
      "\n",
      "Number of unique tokens 22820\n",
      "Number of words found in Glove = 19538\n",
      "X_train vectors\n",
      "(18000, 300)\n",
      "X_test vectors\n",
      "(2000, 300)\n",
      "Training Accuracy - 0.9293888888888889\n",
      "Testing Accuracy - 0.8365\n",
      "Round 5\n",
      "X_Train Shape\n",
      "(18000,)\n",
      "X_Test Shape\n",
      "(2000,)\n",
      "Number of unique tokens 22730\n",
      "\n",
      "Number of unique tokens 22730\n",
      "Number of words found in Glove = 19448\n",
      "X_train vectors\n",
      "(18000, 300)\n",
      "X_test vectors\n",
      "(2000, 300)\n",
      "Training Accuracy - 0.9292777777777778\n",
      "Testing Accuracy - 0.834\n",
      "Round 6\n",
      "X_Train Shape\n",
      "(18000,)\n",
      "X_Test Shape\n",
      "(2000,)\n",
      "Number of unique tokens 22594\n",
      "\n",
      "Number of unique tokens 22594\n",
      "Number of words found in Glove = 19264\n",
      "X_train vectors\n",
      "(18000, 300)\n",
      "X_test vectors\n",
      "(2000, 300)\n",
      "Training Accuracy - 0.9287777777777778\n",
      "Testing Accuracy - 0.847\n",
      "Round 7\n",
      "X_Train Shape\n",
      "(18000,)\n",
      "X_Test Shape\n",
      "(2000,)\n",
      "Number of unique tokens 22903\n",
      "\n",
      "Number of unique tokens 22903\n",
      "Number of words found in Glove = 19577\n",
      "X_train vectors\n",
      "(18000, 300)\n",
      "X_test vectors\n",
      "(2000, 300)\n",
      "Training Accuracy - 0.9277222222222222\n",
      "Testing Accuracy - 0.8345\n",
      "Round 8\n",
      "X_Train Shape\n",
      "(18000,)\n",
      "X_Test Shape\n",
      "(2000,)\n",
      "Number of unique tokens 22755\n",
      "\n",
      "Number of unique tokens 22755\n",
      "Number of words found in Glove = 19476\n",
      "X_train vectors\n",
      "(18000, 300)\n",
      "X_test vectors\n",
      "(2000, 300)\n",
      "Training Accuracy - 0.9288333333333333\n",
      "Testing Accuracy - 0.825\n",
      "Round 9\n",
      "X_Train Shape\n",
      "(18000,)\n",
      "X_Test Shape\n",
      "(2000,)\n",
      "Number of unique tokens 22765\n",
      "\n",
      "Number of unique tokens 22765\n",
      "Number of words found in Glove = 19501\n",
      "X_train vectors\n",
      "(18000, 300)\n",
      "X_test vectors\n",
      "(2000, 300)\n",
      "Training Accuracy - 0.9311666666666667\n",
      "Testing Accuracy - 0.828\n",
      "Round 10\n",
      "X_Train Shape\n",
      "(18000,)\n",
      "X_Test Shape\n",
      "(2000,)\n",
      "Number of unique tokens 22942\n",
      "\n",
      "Number of unique tokens 22942\n",
      "Number of words found in Glove = 19588\n",
      "X_train vectors\n",
      "(18000, 300)\n",
      "X_test vectors\n",
      "(2000, 300)\n",
      "Training Accuracy - 0.9302777777777778\n",
      "Testing Accuracy - 0.83\n",
      "Average Training Accuracy\n",
      "0.9291944444444444\n",
      "Average Testing Accuracy\n",
      "0.8346\n"
     ]
    }
   ],
   "source": [
    "X = df[\"Tokens\"]\n",
    "kf = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "i = 1\n",
    "clf = model # Referencing the previous Support Vector Classifier with the best parameters\n",
    "training_accuracy = []\n",
    "testing_accuracy = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train = X[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_train = y[train_index]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    print(\"Round {}\".format(i))\n",
    "    print(\"X_Train Shape\")\n",
    "    print(X_train.shape)\n",
    "    print(\"X_Test Shape\")\n",
    "    print(X_test.shape)\n",
    "    \n",
    "    all_unique_tokens = list(set([a for b in X_train.tolist() for a in b]))\n",
    "    print(\"Number of unique tokens {}\".format(len(all_unique_tokens)))\n",
    "    print()\n",
    "    \n",
    "    #This will take super long, so instead create the smaller training dictionary from the og_dict\n",
    "#     GLOVE_DATASET_PATH = \"./glove.42B.300d.txt\"\n",
    "#     f = open(GLOVE_DATASET_PATH)\n",
    "#     glove_word_dict = create_glove_vector_dictionary(all_unique_tokens, f)\n",
    "#     f.close()\n",
    "\n",
    "    glove_word_dict = create_train_glove_dictionary(all_unique_tokens)\n",
    "    \n",
    "    print(\"Number of unique tokens {}\".format(len(all_unique_tokens)))\n",
    "    print(\"Number of words found in Glove = {}\".format(len(glove_word_dict.keys())))\n",
    "    \n",
    "    X_train_vectors = X_train.apply(lambda row: glove_vectorizer(row, glove_word_dict))\n",
    "    X_test_vectors = X_test.apply(lambda row: glove_vectorizer(row, glove_word_dict))\n",
    "    \n",
    "    X_train_vectors = np.array(X_train_vectors.tolist())\n",
    "    X_test_vectors = np.array(X_test_vectors.tolist())\n",
    "    \n",
    "    print(\"X_train vectors\")\n",
    "    print(X_train_vectors.shape)\n",
    "    print(\"X_test vectors\")\n",
    "    print(X_test_vectors.shape)\n",
    "    \n",
    "    \n",
    "    clf.fit(X_train_vectors, y_train)\n",
    "    training_accuracy.append(clf.score(X_train_vectors, y_train))\n",
    "    testing_accuracy.append(clf.score(X_test_vectors, y_test))\n",
    "    print(\"Training Accuracy - {}\".format(training_accuracy[i-1]))\n",
    "    print(\"Testing Accuracy - {}\".format(testing_accuracy[i-1]))\n",
    "    i = i + 1\n",
    "print(\"Average Training Accuracy\")\n",
    "print(sum(training_accuracy)/len(training_accuracy))\n",
    "print(\"Average Testing Accuracy\")\n",
    "print(sum(testing_accuracy)/len(testing_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37d4f98",
   "metadata": {},
   "source": [
    "Using the best parameters for SVM, we are doing 10 fold cross validation. \n",
    "Average Training Accuracy\n",
    "0.9291944444444444\n",
    "Average Testing Accuracy\n",
    "0.8346"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d53788ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_training_acc_rf = 0.9748666666666669 * 100\n",
    "glove_testing_acc_rf = 0.7849999999999999 * 100\n",
    "glove_training_acc_lr = 0.7874000000000001 * 100\n",
    "glove_testing_acc_lr = 0.7655500000000001 * 100\n",
    "glove_training_acc_svm = 0.9291944444444444 * 100\n",
    "glove_testing_acc_svm = 0.8346 * 100\n",
    "training_acc = [glove_training_acc_rf, glove_training_acc_lr, glove_training_acc_svm]\n",
    "testing_acc = [glove_testing_acc_rf, glove_testing_acc_lr, glove_testing_acc_svm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0d91e4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = [\"Random Forests\", \"Logistic Regression\", \"Support Vector Machine\"]\n",
    "x = np.arange(len(labels))\n",
    "width = 0.2\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, training_acc, width, label='Training Accuracies')\n",
    "rects2 = ax.bar(x + width/2, testing_acc, width, label='Testing Accuracies')\n",
    "ax.set_ylabel('Accuracies')\n",
    "ax.set_title('Glove Accuracies')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8389a476",
   "metadata": {},
   "source": [
    "Plotting the average training and testing accuracies after 10 Fold Cross Validation for each all the classifiers used. \n",
    "We can see that Support Vector Machine has the best accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e654f8c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "wordvec_testing_acc_rf = 0.74885 * 100\n",
    "wordvec_testing_acc_lr = 0.7414 * 100\n",
    "wordvec_acc_svm = 0.7556999999999999 * 100\n",
    "glove_testing_acc = [glove_testing_acc_rf, glove_testing_acc_lr, glove_testing_acc_svm]\n",
    "wordvec_testing_acc = [wordvec_testing_acc_rf, wordvec_testing_acc_lr, wordvec_acc_svm]\n",
    "labels = [\"Random Forests\", \"Logistic Regression\", \"Support Vector Machine\"]\n",
    "x = np.arange(len(labels))\n",
    "width = 0.2\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, glove_testing_acc, width, label='Glove Accuracies')\n",
    "rects2 = ax.bar(x + width/2, wordvec_testing_acc, width, label='Word2Vec Accuracies')\n",
    "ax.set_ylabel('Accuracies')\n",
    "ax.set_title('Glove vs Word2Vec Accuracies')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.rcParams[\"figure.figsize\"] =(7,7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050da3d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
